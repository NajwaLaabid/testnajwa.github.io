---
title: 'Variational Autoencoders for Modeling Single-Cell Data'
date: 2022-02-04T15:41:03+02:00
permalink: /vae-genomics/
tags:
  - variational inference
  - variational autoencoders
  - single-cell data
---

This is the first of a series of tutorials presenting variational autoencoders (VAEs) to an audience
of bioinformaticians.
This post discusses the motivation behind VAEs and their mathematical basis within the context of modeling single-cell data.
A following tutorial will cover the implementation of a simple VAE for modeling single-cell RNA sequencing (sc-RNA-seq) data as an example.

Some sources that helped make this tutorial are Diederik Kingma's [original paper](https://arxiv.org/abs/1312.6114), Carl Doersch's
[tutorial](https://arxiv.org/abs/1606.05908), Andriy Mnih's Deepmind x UCL [lecture](https://www.youtube.com/watch?v=7Pcvdo4EJeo&ab_channel=DeepMind),
and a number of online blogs focusing on VAEs applied to computer vision ([Keng's](https://bjlkeng.github.io/posts/variational-autoencoders/) and [Jaan's](https://jaan.io/what-is-variational-autoencoder-vae-tutorial/) being my favorite).


## Generative modeling {#generative-modeling}


### Latent space modeling {#latent-space-modeling}

Generative modeling describes the probabilistic process of generating an observation.
One approach to generative modeling is latent space modeling, which assumes that the observed features are generated by a smaller set of
unobserved (i.e. \\(\textit{latent}\\)) variables.
These variables, often abstract and difficult to interpret, are seen as encoding a meaningful internal representation of high dimensional data.
Figure [1](#figure--fig:plate-model) shows a graphical representation of a latent space model in which unobserved variables \\(Z\\) generate observed
variables \\(X\\), and the whole process is repeated \\(N\\) times (to generate \\(N\\) samples).

<a id="figure--fig:plate-model"></a>

{{< figure src="/ox-hugo/plate-model.png" caption="<span class=\"figure-number\">Figure 1: </span>A graphical representation of a latent space model." width="50%" height="50%" >}}

To illustrate the usefulness of latent variables, let us look at the process of studying [biological transcription](https://en.wikipedia.org/wiki/Transcription_(biology)).
We can study the phenotypical differences between groups of cells by comparing
the counts of their expressed genes. If we define a random variable to model the mRNA
counts observed for each gene, we would need to approximate the parameters of around \\(10\\,000\\) marginal distributions
in addition to defining their correlation structure. Not only is this task computationally challenging,
but large variations between transcript counts can be due to small variations in a lower dimensional space.
Instead, we can define 20 latent variables capturing the key directions in which the sequenced cells differ
in their phenotypes. This lower dimensional space is simpler to understand and, if computed well, offers a more
intuitive way of interpreting biological differences compared to the original count space.

From a generative modeling persepctive, we say that these latent variables \\(\textit{generate}\\) the observed measurements.
When modeling the count data within this paradigm, we are interested in learning two things: 1) the latent variables (which are random variables, therefore expressed as probability distributions),
and 2) the parameters of the generative process (which we consider as single point estimates in this tutorial).
Latent variables define a lower dimensional space where we can perform
downstream analysis tasks, like inferring the branching trajectory of cellular development,
cell clustering using UMAP, etc. The generative parameters quantify the process of transcription and
offer us a way to generate count samples similar to the observed ones.

Next, we will look at a mathematical formulation of this modeling paradigm.


### Mathematical formulation of latent space modeling {#mathematical-formulation-of-latent-space-modeling}

We would like to model biological transcription using mRNA transcript counts obtained from sc-RNA-seq.
The input data is an \\(N \times G\\) count matrix \\(D\\), where \\(N\\) is the number of cells sequenced, and \\(G\\) is the number of
genes observed. The matrix typically contains tens of thousands of genes and hundreds of thousands of cells.
We are interested in modeling the count of transcripts per gene across cells.
We therefore define a vector of random variables \\(X\\) where each variable represents the counts per one gene.
We would like to learn the parameters of the multi-variate distribution of \\(X\\).
We call \\(Z\\) the vector of latent variables used in this model.

As mentioned earlier, we are interested in learning: 1) the distribution of latent variables \\(Z\\),
and 2) the parameters of the generating distribution \\(P(X)\\).

For the first task, we can use Bayes rule to infer the posterior, as shown in equation \\(\ref{eq:posterior}\\):

\begin{equation}
\label{eq:posterior}
      p\_{\theta}(Z|X) = \frac{p\_{\theta}(X|Z)p\_{\theta}(Z)}{p\_{\theta}(X)}
\end{equation}

\begin{equation}
\label{eq:evidence}
      p\_{\theta}(X) = \int p\_{\theta}(X|Z) p\_{\theta}(Z) dZ
\end{equation}

\\(p\_{\theta}(X|Z)\\) is known as the likelihood. We know it from the data. \\(p\_{\theta}(Z)\\) is called the prior.
It captures any assumptions we have about the abstract latent space. It is defined as part of the model assumptions.
\\(p\_{\theta}(X)\\) is the marginal likelihood or evidence. It can be computed from the likelihood and prior
using the law of total probability (equation \\(\ref{eq:evidence}\\)). Parameters \\(\theta\\) characterize the generative process. We would like
to learn them alongside the posterior \\(p\_{\theta}(Z|X)\\). Note that learning the posterior in this context means generating a distribution over possible
latent values \\(Z\\) for each data point \\(X\\).

Unfortunately, \\(p\_{\theta}(X)\\) only has a closed form for conjugate likelihoods
(i.e., likelihoods which when multiplied by a given prior, return a posterior of the same family as the prior).
Gaussian likelihoods and priors are an example of this scenario. However, Gaussian likelihoods are not always a good fit
for real world situations. For example, sc-RNA-seq data is better fitted with a Negative Binomial (NB)
distribution to account for the overdispersion of the counts per gene. The NB distribution does not have a conjugate prior \\(p\_{\theta}(Z)\\),
which makes equation \\(\ref{eq:evidence}\\) and consequently equation \\(\ref{eq:posterior}\\) intractable.
An alternative to computing the posterior analytically is to approximate it, as discussed in the next section.


### Variational Bayes for approximating the posterior {#variational-bayes-for-approximating-the-posterior}

Variational inference is one popular approach for approximating the posterior.
It is an alternative to sampling based methods, like [Monte Carlo](https://en.wikipedia.org/wiki/Monte_Carlo_method), which often require multiple \\(Z\\) samples per data point
and therefore do not scale to large datasets.

Variational Inference aims to approximate the posterior as closely as possible by a distribution \\(q\_{\phi}(Z|X)\\)
(sometimes known as a \\(\textit{recognition model}\\) or \\(\textit{variational distribution}\\)).
We can learn the parameters \\(\phi\\) (called \\(\textit{variational parameters}\\)) alongside the generative parameters \\(\theta\\).
This closeness between the two distributions is captured by the [Kullback-Liebler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) as shown in equation \\(\ref{eq:kl\_divergence}\\):

\begin{equation}
\label{eq:kl\_divergence}
\begin{aligned}
   D[q\_{\phi}(Z|X)||p\_{\theta}(Z|X)] &= \int q\_{\phi}(Z|X) log[\frac{q\_{\phi}(Z|X)}{p\_{\theta}(Z|X)}] dZ \\\\
                                     &= E\_{Z \sim q\_{\phi}} [log \\, q\_{\phi}(Z|X) - log \\, p\_{\theta}(Z|X)]  \\\\
\end{aligned}
\end{equation}

We replace \\(p\_{\theta}(Z|X)\\) using Bayes rule (recall equation \\(\ref{eq:posterior}\\)):

\begin{equation}
\label{eq:kl\_divergence\_2}
\begin{aligned}
 D[q\_{\phi}(Z|X)||p\_{\theta}(Z|X)]   &= E\_{Z \sim q\_{\phi}} [log \\, q\_{\phi}(Z|X) - log \\, p\_{\theta}(X|Z) - log \\, p\_{\theta}(Z) + log \\, p\_{\theta}(X)]  \\\\
                                     &= E\_{Z \sim q\_{\phi}} [log \\, q\_{\phi}(Z|X) - log \\, p\_{\theta}(X|Z) - log \\, p\_{\theta}(Z)] + log \\, p\_{\theta}(X)  \\\\
                                     &= E\_{Z \sim q\_{\phi}} [log Q(Z|X) - log \\, p\_{\theta}(X|Z) - log \\, p\_{\theta}(Z)] + log \\, p\_{\theta}(X)  \\\\
\end{aligned}
\end{equation}

We rearrange equation \\(\ref{eq:kl\_divergence\_2}\\) using the expected value form of the KL-divergence on the last line to get:

\begin{equation}
\label{eq:objective\_simplified}
\begin{aligned}
   log \\, p\_{\theta}(X) - D[q\_{\phi}(Z|X)||p\_{\theta}(Z|X)] &= - E\_{Z \sim q\_{\phi}} [log \\, q\_{\phi}(Z|X) - log \\, p\_{\theta}(X|Z) - log \\, p\_{\theta}(Z)] \\\\
                                                         &= E\_{Z \sim q\_{\phi}} [log \\, p\_{\theta}(X|Z) + log \\, p\_{\theta}(Z) - log \\, q\_{\phi}(Z|X)] \\\\
                                                         &= E\_{Z \sim q\_{\phi}} [log \\, p\_{\theta}(X|Z)] - E\_{Z \sim q\_{\phi}}[log \\, q\_{\phi}(Z|X) - log \\, p\_{\theta}(Z)] \\\\
                                                         &= E\_{Z \sim q\_{\phi}} [log \\, p\_{\theta}(X|Z)] - D[q\_{\phi}(Z|X)||p\_{\theta}(Z)] \\\\
\end{aligned}
\end{equation}

The left hand side (LHS) of equation \\(\ref{eq:objective\_simplified}\\) combines the two learning objectives of interest:
maximizing \\(log \\, p\_{\theta}(X)\\) to learn the generating parameters, and minimizing the KL-divergence \\(D[q\_{\phi}(Z|X)||p\_{\theta}(Z|X)]\\) to approximate the posterior.
The right-hand-side (RHS) offers an equivalent expression that does not depend on the intractable integral and can therefore
be learnt using gradient-based algorithms.

We also note here the apparition of the encoding/decoding elements from which the method derives its name.
The probabilistic \\(\textit{encoder} \\, q\_{\phi}(Z|X)\\) creates a latent code \\(Z\\) for a given observed data \\(X\\).
Similarly, the \\(\textit{decoder} \\, p\_{\theta}(X|Z)\\) translates a given code \\(Z\\) back to the original feature space.

The RHS of equation \\(\ref{eq:objective\_simplified}\\) is often referred to as the Evidence Lower Bound (ELBO),
symbolized by \\(\mathcal{L}(\theta, \phi; X)\\).
Since \\(D[q\_{\phi}(Z|X)||p\_{\theta}(Z|X)] > 0\\),
\\(log \\, p\_{\theta}(X) > \mathcal{L}(\theta, \phi; X) = E\_{Z \sim q\_{\phi}} [log \\, p\_{\theta}(X|Z)] - D[q\_{\phi}(Z|X)||p\_{\theta}(Z)]\\).

Variational autoencoders offer a way to optimize this objective using neural networks and the reparametrization trick,
which we review below.


## Variational Autoencoders {#variational-autoencoders}


### Auto-encoding Variational Bayes (AEVB) Algorithm {#auto-encoding-variational-bayes--aevb--algorithm}

The main task here is to optimize the variational lower bound \\(\mathcal{L}(\theta, \phi ; X)\\) with respect to both \\(\theta\\) and \\(\phi\\).
The second term of the RHS, \\(D[q\_{\phi}(Z|X)||p\_{\theta}(Z)]\\), has a closed-form solution
if we take both \\(q\_{\phi}(Z|X)\\) and \\(p\_{\theta}(Z)\\) to be Gaussian.
This condition is easy to meet since we choose both the prior and the variational distribution ourselves.
This means we can take the derivative of this term and optimize it via stochastic gradient descent.

Finding a suitable estimator for \\(E\_{Z \sim q\_{\phi}} log \\, p\_{\theta}(X|Z)\\) is a little trickier.
The expectation with respect to \\(q\_{\phi}\\) has a high variance (see here for a [demonstration](https://nbviewer.org/github/gokererdogan/Notebooks/blob/master/Reparameterization%20Trick.ipynb)).
To resolve this issue, AEVB proposes a \\(\textit{reparametrization trick}\\) to make the expectation independent of the parameters \\(\phi\\),
the target of the optimization.
The trick is based on the observation that we can transform samples from one distribution to another one via a deterministic function.

\begin{equation}
\label{eq:reparam\_trick}
   Z \sim q\_{\phi}(Z) \rightarrow Z = g\_{\phi}(\epsilon, X)
\end{equation}

with \\(\epsilon \sim p(\epsilon)\\).
The simplest example of this transformation is shifting and scaling a standard Normal distribution:

\begin{equation}
\label{eq:gaussian\_example}
   Z \sim \mathcal{N}(\mu, \sigma) \rightarrow Z  = \mu + \sigma \* \epsilon
\end{equation}

for \\(\epsilon \sim \mathcal{N}(0, I)\\). With this transformation in mind, we can write the expectation \\(E\_{q\_{\phi}(Z|X)} [log \\, p\_{\theta}(X|Z)]\\)
with respect to a distribution \\(p(\epsilon)\\) independent of \\(\phi\\).

\begin{equation}
\label{eq:rewrite\_expectation}
  E\_{q\_{\phi}(Z|X)} [log \\, p\_{\theta}(X|Z)] \approx E\_{p(\epsilon)} [f(g\_{\phi}(\epsilon, X)]
\end{equation}

where \\(\epsilon \sim p(\epsilon)\\) and \\(f(Z)\\) is \\(log \\, p\_{\theta}(X|Z)\\).
The new expectation can be approximated by:

\begin{equation}
\label{eq:expectation\_sample}
   E\_{p(\epsilon)}[log \\, p\_{\theta}(X|Z)] = \frac{1}{L} \sum\_{l=1}^{L} log \\, p\_{\theta}(X|Z)
\end{equation}

The final expression of the lower bound, with the transformed \\(Z\\), would therefore be:

\begin{equation}
\label{eq:final\_elbo}
   \mathcal{L}(\theta, \phi; X) = - D[q\_{\phi}(Z|X)||p\_{\theta}(Z)] + \frac{1}{L} \sum\_{l=1}^{L} log , p\_{\theta}(X|Z)
\end{equation}

with \\(Z = g\_{\phi}(\epsilon, X)\\) and \\(\epsilon \sim p(\epsilon)\\).


### The Encoder and Decoder as Neural Networks {#the-encoder-and-decoder-as-neural-networks}

<a id="figure--fig:training-vae"></a>

{{< figure src="/ox-hugo/reparam-trick.png" caption="<span class=\"figure-number\">Figure 2: </span>A diagram of a variational auto-encoder ([source](https://bjlkeng.github.io/posts/variational-autoencoders/))." >}}

Variational autoencoders are a special instantiation of the AEVB algorithm in which we use neural networks to implement the encoder
\\(q\_{\phi}(Z|X)\\) and decoder \\(p\_{\theta}(X|Z)\\) models.

Figure [2](#figure--fig:training-vae) illustrates this idea. We start with the data \\(X\\), and learn the parameters \\(\phi\\)
of the encoding distribution (i.e. approximate posterior) \\(q\_{\phi}(Z|X)\\) by minimizing the KL divergence between \\(q\_{\phi}(Z|X)\\)
and the prior \\(p\_{\theta}(Z)\\). We then sample a \\(Z\\) from the approximate posterior. This \\(Z\\) is used by the decoder to estimate the parameters
\\(\theta\\) of \\(p\_{\theta}(X|Z)\\). In this example, we use mean squared error (MSE) to optimize the decoder because we assume \\(p\_{\theta}(X|Z)\\)
follows a multi-variate Gaussian distribution.

Notice that, thanks to the reparametrization trick, we are able to replace sampling by a deterministic function.
We take a sample from a standard normal distribution in the input layer, and shift and scale it using the parameters
learnt by the encoder model. Without the reparameterization trick, we would not be able to backpropagate the derivative of the MSE term
through the sampling layer, and the training of the neural networks via backpropagation would fail.


## Summary {#summary}

In this tutorial, we covered how generative modeling is useful in defining a lower dimensional space to better understand the variation in single-cell sequencing data.
We also explained the main learning goals when using generative models, namely: 1) infering latent variables
and 2) estimating the parameters of the generative process. We then showed how the AEVB algorithm achieves both goals by defining a low-variance estimator
of the ELBO. This estimator can be trained via stochastic gradient descent thanks to the reparametrization trick.
Finally, we presented variational auto-encoders as an instantiation of AEVB which uses neural networks to implement the encoder and decoder
models.

In the next tutorial, we will see a practical example of how to implement a simple VAE to model sc-RNA-seq data, and how the
distributions recovered from the model can be used for all kinds of downstream biological analyses.


## VAEs in single-cell analysis literature {#vaes-in-single-cell-analysis-literature}

Below is a non-exhaustive list of methods using VAEs to analyze single-cell data:

-   [scVI](https://www.nature.com/articles/s41592-018-0229-2) proposes a VAE model to analyze sc-RNA-seq data.
-   [totalVI](https://www.nature.com/articles/s41592-020-01050-x) extends scVI to modeling multi-modal RNA and protein datasets.
-   [scVAE](https://github.com/scvae/scvae) also focuses on modeling sc-RNA-seq data, but it includes the cell size as a modeling variable.
-   [SISUA](https://www.biorxiv.org/content/10.1101/631382v1) uses a semi-supervised VAE model to build latent space representations incorporating both protein and RNA data.